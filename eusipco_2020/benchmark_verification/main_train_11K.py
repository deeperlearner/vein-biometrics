"""
This code is generated by Ridvan Salih KUZU @UNIROMA3
LAST EDITED:  02.03.2020
ABOUT SCRIPT:
It is a main script for training a verification system for a given database.
It considers CUSTOM PENALTY functions.
The related databases should be placed in data_bosphorus, data_polyup, and data_sdumla folders

"""
import logging
import sys
import argparse
import shutil
import os
from itertools import chain
import csv
import math

import pandas as pd
import numpy as np
import torch
from torch.optim import lr_scheduler
from torch.nn import CrossEntropyLoss
from torchvision import transforms
from torch.utils.tensorboard import SummaryWriter
from torchvision.utils import save_image, make_grid
from prettytable import PrettyTable
from tqdm import tqdm
import PIL.Image

from utils import FullPairComparer, AverageMeter, evaluate, plot_roc, plot_DET_with_EER, plot_density, accuracy
from models import MNASNet_Modified as net
from benchmark_verification.hands_loader import get_dataloader
from losses import *


parser = argparse.ArgumentParser(description='Vein Verification')

parser.add_argument('--start-epoch', default=0, type=int, metavar='SE',
                    help='start epoch (default: 0)')
parser.add_argument('--num-epochs', default=1, type=int, metavar='NE',
                    help='number of epochs to train (default: 90)')
parser.add_argument('--num-classes', default=200, type=int, metavar='NC',
                    help='number of clases (default: 318)')
parser.add_argument('--embedding-size', default=512, type=int, metavar='ES',
                    help='embedding size (default: 128)')
parser.add_argument('--batch-size', default=32, type=int, metavar='BS',
                    help='batch size (default: 128)')
parser.add_argument('--num-workers', default=16, type=int, metavar='NW',
                    help='number of workers (default: 8)')
parser.add_argument('--learning-rate', default=0.01, type=float, metavar='LR',
                    help='learning rate') #seems best when SWG off
parser.add_argument('--weight-decay', default=0.0001, type=float, metavar='WD',
                    help='weight decay (default: 0.01)') #seems best when SWG off
parser.add_argument('--scale-rate', default=32, type=float, metavar='SC',
                    help='scale rate (default: 0.001)')
parser.add_argument('--margin', default=0.5, type=float, metavar='MG',
                    help='margin')
parser.add_argument('--database-dir', default='/media/back/home/chuck/11K_Hands_processed', type=str,
                    help='path to the database root directory')
parser.add_argument('--train-dir', default='Contactless_Knuckle_Palm_Print_and_Vein_Dataset/CSVFiles/train_dis.csv', type=str,
                    help='path to train root dir')
parser.add_argument('--valid-dir', default='Contactless_Knuckle_Palm_Print_and_Vein_Dataset/CSVFiles/val_dis.csv', type=str,
                    help='path to valid root dir')
parser.add_argument('--test-dir', default='Contactless_Knuckle_Palm_Print_and_Vein_Dataset/CSVFiles/test_pairs_dis.csv', type=str,
                    help='path to test root dir')
parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',
                    help='evaluate model on test set')
parser.add_argument('--type', default='aamp', type=str, metavar='MG',
                    help='type')
parser.add_argument('--outdir', default='model_11K/', type=str,
                    help='Out Directory')
parser.add_argument('--logdir', default='logs.csv', type=str,
                    help='path to log dir')

args = parser.parse_args()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
global best_losss
best_losss = 100
global best_test
best_test = 100

log_root = logging.getLogger()
log_root.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s-%(message)s")
handler_file = logging.FileHandler(os.path.join(args.outdir, "training.log"))
handler_stream = logging.StreamHandler(sys.stdout)
handler_file.setFormatter(formatter)
handler_stream.setFormatter(formatter)
log_root.addHandler(handler_file)
log_root.addHandler(handler_stream)

summary_writer = SummaryWriter(log_dir=os.path.join(args.outdir, "tensorboard"))

def main():
    if not os.path.exists(args.outdir):
        os.makedirs(args.outdir)

    criterion = CrossEntropyLoss().cuda()
    if args.type == 'norm':
        loss_metric = NormSoftmax(args.embedding_size, args.num_classes, args.scale_rate).cuda()
    elif args.type == 'aamp':
        loss_metric = ArcMarginProduct(args.embedding_size, args.num_classes, s=args.scale_rate, m=args.margin).cuda()
    elif args.type == 'lmcp':
        loss_metric = AddMarginProduct(args.embedding_size, args.num_classes, s=args.scale_rate, m=args.margin).cuda()
    elif args.type == 'sphere':
        loss_metric = SphereProduct(args.embedding_size, args.num_classes, m=int(args.margin)).cuda()
    elif args.type == 'lgm':
        loss_metric = CovFixLGM(args.embedding_size, args.num_classes, args.margin).cuda()
    elif args.type == 'lgm2':
        loss_metric = LGMLoss(args.embedding_size, args.num_classes, args.margin).cuda()
    elif args.type == 'none':
        loss_metric = None

    if loss_metric is not None:
        model = net(embedding_size=args.embedding_size, class_size=args.num_classes, only_embeddings=True, pretrained=True)
        to_be_optimized = chain(model.parameters(), loss_metric.parameters())
    else:
        model = net(embedding_size=args.embedding_size, class_size=args.num_classes, only_embeddings=False, pretrained=True)
        to_be_optimized = model.parameters()

    model = torch.nn.DataParallel(model).cuda()
    # optimizer = torch.optim.Adam(to_be_optimized,
    #                              lr=args.learning_rate,
    #                              weight_decay=args.weight_decay)
    optimizer = torch.optim.SGD(to_be_optimized,
                                lr=args.learning_rate,
                                momentum=0.9,
                                weight_decay=args.weight_decay)


    #scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, cooldown=2, verbose=True)

    if args.start_epoch != 0:
        checkpoint = torch.load(args.outdir+'/model_checkpoint.pth.tar')
        args.start_epoch = checkpoint['epoch']
        model.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer'])

    if args.evaluate:
        checkpoint = torch.load(args.outdir + '/model_best.pth.tar')
        args.start_epoch = checkpoint['epoch']
        model.load_state_dict(checkpoint['state_dict'], strict=False)
        optimizer.load_state_dict(checkpoint['optimizer'])
        # data_loaders = get_dataloader(args.database_dir, args.train_dir, args.valid_dir, args.test_dir,
        #                               args.batch_size, args.num_workers)
        # test(model, data_loaders['test'], '00', is_graph=True)
    else:
        data_loaders = get_dataloader(args.database_dir, batch_size=args.batch_size)

        for epoch in range(args.start_epoch, args.num_epochs + args.start_epoch):
            print(80 * '=')
            print('Epoch [{}/{}]'.format(epoch, args.num_epochs + args.start_epoch - 1))


            train(model, optimizer, epoch, data_loaders["train"], criterion, loss_metric)
            is_best, acc, loss = validate(model, optimizer, epoch, data_loaders["valid"], criterion, loss_metric)
            scheduler.step(loss)
            lr = scheduler._last_lr[0]
            # if lr < 1E-5:
            #     break
            if is_best and acc > 100:
                test(model, data_loaders["test"], epoch, is_graph=True)

        # epoch = 99
        print(80 * '=')

        ## MODEL EVALUATION LOGGING ##
        checkpoint = torch.load(args.outdir + '/model_best.pth.tar')
        model.load_state_dict(checkpoint['state_dict'])
        EER = test(model, data_loaders["test"], epoch, is_graph=True)

        header = ['weight_decay', 'learning_rate', 'scale', 'margin', 'type', 'batch_size', 'embedding_size', 'EER', 'out_dir']
        info = [args.weight_decay, args.learning_rate, args.scale_rate, args.margin, args.type, args.batch_size, args.embedding_size, EER, args.outdir]

        if not os.path.exists(args.logdir):
            with open(args.logdir, 'w') as file:
                logger = csv.writer(file)
                logger.writerow(header)
                logger.writerow(info)
        else:
            with open(args.logdir, 'a') as file:
                logger = csv.writer(file)
                logger.writerow(info)


def test(model, dataloader, epoch, is_graph=False):
    global best_test
    labels, distances = [], []
    im_size = (128, 128)
    trans = transforms.Compose([transforms.Resize(im_size),
                                transforms.ToTensor()])
    with torch.set_grad_enabled(False):
        comparer = FullPairComparer().cuda()
        model.eval()
        for batch_idx, (data1, data2, target, im_file1, im_file2) in enumerate(tqdm(dataloader)):
            target = target.cuda(non_blocking=True)

            output1 = model(data1, False)
            output2 = model(data2, False)
            dist = comparer(output1, output2) #TODO: sign - torch.sign()
            # print(target[:4])
            # print(dist[:4])
            indices = torch.where((target == 1) & (dist < 0.3))[0]
            if len(indices):
                files1 = [im_file1[i] for i in indices.tolist()]
                im1s = []
                for f in files1:
                    im1 = PIL.Image.open(f).convert('RGB')
                    im1 = trans(im1)
                    im1s.append(im1)
                    if len(im1s) == 8:
                        break
                im1s = torch.stack(im1s)
                summary_writer.add_image("genuine1", im1s, batch_idx, dataformats='NCHW')

                files2 = [im_file2[i] for i in indices.tolist()]
                im2s = []
                for f in files2:
                    im2 = PIL.Image.open(f).convert('RGB')
                    im2 = trans(im2)
                    im2s.append(im2)
                    if len(im2s) == 8:
                        break
                im2s = torch.stack(im2s)
                summary_writer.add_image("genuine2", im2s, batch_idx, dataformats='NCHW')
                # genuine_data1 = data1[indices]
                # genuine_data2 = data2[indices]
                # summary_writer.add_image("high dist genuine1", make_grid(genuine_data1, nrow=8), batch_idx)
                # summary_writer.add_image("high dist genuine2", make_grid(genuine_data2, nrow=8), batch_idx)
            #dist = comparer(torch.sign(F.relu(output1)), torch.sign(F.relu(output2)))  # TODO: sign - torch.sign()
            distances.append(dist.data.cpu().numpy())
            labels.append(target.data.cpu().numpy())
            # if batch_idx % 50 == 0:
            #     print('Batch-Index -{}'.format(str(batch_idx)))

    labels = np.array([sublabel for label in labels for sublabel in label])
    distances = np.array([subdist for dist in distances for subdist in dist])
    tpr, fpr, fnr, fpr_optimum, fnr_optimum, accuracy, threshold, thresholds = evaluate(distances, labels)
    print(labels[0:4])
    print(distances[0:4])

    EER = np.mean(fpr_optimum + fnr_optimum) / 2
    print('TEST - Accuracy           = {:.12f}'.format(accuracy))
    print('TEST - EER                = {:.12f}'.format(EER))
    print('Best threshold            = {:.12f}'.format(threshold))
    is_best = EER <= best_test
    best_test = min(EER, best_test)

    if is_best and is_graph:
        plot_roc(fpr, tpr, figure_name=args.outdir + '/Test_ROC-{}.png'.format(epoch))

        #######################################################################
        x_labels = [10 ** -6, 10 ** -5, 10 ** -4, 10 ** -3, 10 ** -2, 10 ** -1]
        tpr_fpr_table = PrettyTable(['Methods'] + [str(x) for x in x_labels])
        tpr_fpr_row = []
        tpr_fpr_row.append("MNASNet")
        for fpr_iter in np.arange(len(x_labels)):
            _, min_index = min(
                list(zip(abs(fpr - x_labels[fpr_iter]), range(len(fpr)))))
            tpr_fpr_row.append('%.2f' % (tpr[min_index] * 100))
            print(f'tpr={tpr[min_index]},fpr={fpr[min_index]},threshold={thresholds[min_index]}')
        tpr_fpr_table.add_row(tpr_fpr_row)
        print(tpr_fpr_table)
        #######################################################################

        plot_DET_with_EER(fpr, fnr, fpr_optimum, fnr_optimum,
                          figure_name=args.outdir + '/Test_DET-{}.png'.format(epoch))
        plot_density(distances, labels, figure_name=args.outdir + '/Test_DENSITY-{}.png'.format(epoch))
        df_results = pd.DataFrame({'distances': distances.transpose(), 'labels': labels.transpose()})
        df_results.to_csv(args.outdir + "/test_outputs.csv", index=False)

        if args.evaluate is False:
            shutil.copyfile(args.outdir + '/model_best.pth.tar', args.outdir + '/test_model_best.pth.tar')

    return EER


def train(model, optimizer, epoch, dataloader, criterion, metric):
    with torch.set_grad_enabled(True):
        losses = AverageMeter()
        top1 = AverageMeter()
        model.train()

        for batch_idx, (data, target, _) in enumerate(dataloader):
            optimizer.zero_grad()
            target = target.cuda()
            outputs = model(data.cuda())
            if metric is not None:
                outputs = metric(outputs, target)
            loss = criterion(outputs, target)
            # print(loss.item())
            # if torch.isnan(loss):
            #     print("there is nan!!!!")
            #     print(loss)
            #     print(model(data.cuda()))
            #     print(outputs)
            #     # for name, param in model.named_parameters():
            #     #     print(name, param.grad.norm())
            #     os._exit(1)

            prec1, prec5 = accuracy(outputs, target, topk=(1, 5))
            losses.update(loss.item(), data.size(0))
            top1.update(prec1[0], data.size(0))
            loss.backward()
            optimizer.step()
            if batch_idx % 25 == 0:
                print('Step-{} Prec@1 {top1.avg:.5f} loss@1 - {loss.avg:.5f}'.format(batch_idx, top1=top1, loss=losses))

        logging.info('*TRAIN* Epoch {epoch}: Prec@1 {top1.avg:.5f} - loss@1 {loss.avg:.5f}'.format(epoch=epoch, top1=top1, loss=losses))
        summary_writer.add_scalar("Train accuracy", top1.avg, global_step=epoch, )
        summary_writer.add_scalar("Train loss", losses.avg, global_step=epoch, )

def validate(model, optimizer, epoch, dataloader, criterion, metric):
    global best_losss
    with torch.set_grad_enabled(False):
        losses = AverageMeter()
        top1 = AverageMeter()

        model.eval()

        for batch_idx, (data, target, _) in enumerate(dataloader):
            target = target.cuda(non_blocking=True)
            outputs = model(data.cuda())
            if metric is not None:
                outputs = metric(outputs, target)
            loss = criterion(outputs, target)
            prec1, prec5 = accuracy(outputs, target, topk=(1, 5))
            losses.update(loss.item(), data.size(0))
            top1.update(prec1[0], data.size(0))
            if batch_idx % 5 == 0:
                print('Step-{} Prec@1 {top1.avg:.5f} loss@1 - {loss.avg:.5f}'.format(batch_idx, top1=top1, loss=losses))

        logging.info('*VALID* Epoch {epoch}: Prec@1 {top1.avg:.5f} - loss@1 {loss.avg:.5f}'.format(epoch=epoch, top1=top1, loss=losses))
        summary_writer.add_scalar("Valid accuracy", top1.avg, global_step=epoch, )
        summary_writer.add_scalar("Valid loss", losses.avg, global_step=epoch, )

        is_best = losses.avg <= best_losss
        best_losss = min(losses.avg, best_losss)
        torch.save({'epoch': epoch+1,
                    'state_dict': model.state_dict(),
                    'optimizer': optimizer.state_dict()},
                   args.outdir+'/model_checkpoint.pth.tar')
        if is_best:
            shutil.copyfile(args.outdir+'/model_checkpoint.pth.tar', args.outdir+'/model_best.pth.tar')

        return is_best, top1.avg, losses.avg


if __name__ == '__main__':
    main()
