"""
This code is generated by Ridvan Salih KUZU @UNIROMA3
LAST EDITED:  02.03.2020
ABOUT SCRIPT:
It is a main script for training a verification system for a given database.
It considers CUSTOM PENALTY functions.
The related databases should be placed in data_bosphorus, data_polyup, and data_sdumla folders

"""
import argparse
import shutil
import os
from itertools import chain
import csv
import math

import pandas as pd
import numpy as np
import torch
from torch.optim import lr_scheduler
from torch.nn import CrossEntropyLoss
import learn2learn as l2l

from utils import FullPairComparer, AverageMeter, evaluate, plot_roc, plot_DET_with_EER, plot_density, accuracy
# from models import MNASNet_Modified as net
from models import PalmCNN as net
from benchmark_verification import get_dataset, get_dataloader
from losses import *

parser = argparse.ArgumentParser(description='Vein Verification')

parser.add_argument('--N-way', default=5, type=int, help='number of way')
parser.add_argument('--K-shot', default=1, type=int, help='number of shot')

parser.add_argument('--start-epoch', default=0, type=int, metavar='SE',
                    help='start epoch (default: 0)')
parser.add_argument('--num-epochs', default=1, type=int, metavar='NE',
                    help='number of epochs to train (default: 90)')
parser.add_argument('--num-classes', default=200, type=int, metavar='NC',
                    help='number of clases (default: 318)')
parser.add_argument('--embedding-size', default=512, type=int, metavar='ES',
                    help='embedding size (default: 128)')
parser.add_argument('--batch-size', default=32, type=int, metavar='BS',
                    help='batch size (default: 128)')
parser.add_argument('--num-workers', default=16, type=int, metavar='NW',
                    help='number of workers (default: 8)')
parser.add_argument('--learning-rate', default=0.01, type=float, metavar='LR',
                    help='learning rate') #seems best when SWG off
parser.add_argument('--weight-decay', default=0.0001, type=float, metavar='WD',
                    help='weight decay (default: 0.01)') #seems best when SWG off
parser.add_argument('--scale-rate', default=32, type=float, metavar='SC',
                    help='scale rate (default: 0.001)')
parser.add_argument('--margin', default=0.5, type=float, metavar='MG',
                    help='margin')
parser.add_argument('--database-dir', default='Contactless_Knuckle_Palm_Print_and_Vein_Dataset', type=str,
                    help='path to the database root directory')
parser.add_argument('--train-dir', default='Contactless_Knuckle_Palm_Print_and_Vein_Dataset/CSVFiles_v1/train_dis.csv', type=str,
                    help='path to train root dir')
parser.add_argument('--valid-dir', default='Contactless_Knuckle_Palm_Print_and_Vein_Dataset/CSVFiles_v1/val_dis.csv', type=str,
                    help='path to valid root dir')
parser.add_argument('--test-dir', default='Contactless_Knuckle_Palm_Print_and_Vein_Dataset/CSVFiles/test_pairs_dis.csv', type=str,
                    help='path to test root dir')
parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',
                    help='evaluate model on test set')
parser.add_argument('--type', default='aamp', type=str, metavar='MG',
                    help='type')
parser.add_argument('--outdir', default='palm_model/', type=str,
                    help='Out Directory')
parser.add_argument('--logdir', default='logs.csv', type=str,
                    help='path to log dir')

args = parser.parse_args()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
global best_losss
best_losss = 100
global best_test
best_test = 100


def meta_accuracy(predictions, targets):
    predictions = predictions.argmax(dim=1)
    acc = (predictions == targets).sum().float()
    acc /= len(targets)
    return acc


def fast_adapt(batch, learner, criterion, fas):
    ways, shots = args.N_way, args.K_shot

    data, labels = batch
    data = data.to(device)
    labels = labels.to(device)

    # Separate data into adaptation/evalutation sets
    adaptation_indices = np.zeros(data.size(0), dtype=bool)
    adaptation_indices[np.arange(shots*ways) * 2] = True
    evaluation_indices = torch.from_numpy(~adaptation_indices)
    adaptation_indices = torch.from_numpy(adaptation_indices)
    adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]
    evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]

    # Adapt the model
    for step in range(fas):
        adaption_error = criterion(learner(adaptation_data), adaptation_labels)
        learner.adapt(adaption_error)

    # Evaluate the adapted model
    predictions = learner(evaluation_data)
    evaluation_error = criterion(predictions, evaluation_labels)
    evaluation_error /= len(evaluation_data)
    evaluation_accuracy = meta_accuracy(predictions, evaluation_labels)
    return evaluation_error, evaluation_accuracy


def main():
    if not os.path.exists(args.outdir):
        os.makedirs(args.outdir)

    criterion = CrossEntropyLoss().cuda()
    if args.type == 'norm':
        loss_metric = NormSoftmax(args.embedding_size, args.num_classes, args.scale_rate).cuda()
    elif args.type == 'aamp':
        loss_metric = ArcMarginProduct(args.embedding_size, args.num_classes, s=args.scale_rate, m=args.margin).cuda()
    elif args.type == 'lmcp':
        loss_metric = AddMarginProduct(args.embedding_size, args.num_classes, s=args.scale_rate, m=args.margin).cuda()
    elif args.type == 'sphere':
        loss_metric = SphereProduct(args.embedding_size, args.num_classes, m=int(args.margin)).cuda()
    elif args.type == 'lgm':
        loss_metric = CovFixLGM(args.embedding_size, args.num_classes, args.margin).cuda()
    elif args.type == 'lgm2':
        loss_metric = LGMLoss(args.embedding_size, args.num_classes, args.margin).cuda()
    elif args.type == 'none':
        loss_metric = None

    if loss_metric is not None:
        # model = net(embedding_size=args.embedding_size, class_size=args.num_classes, only_embeddings=True, pretrained=True)
        model = net(embedding_size=args.embedding_size)
        to_be_optimized = chain(model.parameters(), loss_metric.parameters())
    else:
        model = net(embedding_size=args.embedding_size, class_size=args.num_classes, only_embeddings=False, pretrained=True)
        to_be_optimized = model.parameters()

    model = torch.nn.DataParallel(model).cuda()
    # optimizer = torch.optim.Adam(to_be_optimized,
    #                              lr=args.learning_rate,
    #                              weight_decay=args.weight_decay)
    optimizer = torch.optim.SGD(to_be_optimized,
                                lr=args.learning_rate,
                                momentum=0.9,
                                weight_decay=args.weight_decay)


    #scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, cooldown=2, verbose=True)

    if args.start_epoch != 0:
        checkpoint = torch.load(args.outdir+'/model_checkpoint.pth.tar')
        args.start_epoch = checkpoint['epoch']
        model.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer'])
    if args.evaluate:
        checkpoint = torch.load(args.outdir + '/model_best.pth.tar')
        args.start_epoch = checkpoint['epoch']
        model.load_state_dict(checkpoint['state_dict'], strict=False)
        optimizer.load_state_dict(checkpoint['optimizer'])
        data_loaders = get_dataloader(args.database_dir, args.train_dir, args.valid_dir, args.test_dir,
                                      args.batch_size, args.num_workers)
        test(model, data_loaders['test'], '00', is_graph=True)

    else:
        # meta learning hyper-parameters
        maml_lr = 0.01
        iterations = 1000
        tps = 32
        fas = 5

        datasets = get_dataset(args.database_dir, args.train_dir, args.valid_dir, args.test_dir)
        train_set = l2l.data.MetaDataset(datasets['train'])
        train_tasks = l2l.data.Taskset(
            train_set,
            task_transforms=[
                l2l.data.transforms.NWays(train_set, args.N_way),
                l2l.data.transforms.KShots(train_set, 2*args.K_shot),
                l2l.data.transforms.LoadData(train_set),
                l2l.data.transforms.RemapLabels(train_set),
                l2l.data.transforms.ConsecutiveLabels(train_set),
            ]
        )
        meta_model = l2l.algorithms.MAML(model, lr=maml_lr)
        # opt = torch.optim.Adam(meta_model.parameters(), lr=args.learning_rate)
        opt = torch.optim.SGD(meta_model.parameters(),
                              lr=args.learning_rate,
                              momentum=0.9,
                              weight_decay=args.weight_decay)
        criterion
        for iteration in range(iterations):
            iteration_error = 0.0
            iteration_acc = 0.0
            for _ in range(tps):
                learner = meta_model.clone()
                batch = train_tasks.sample()

                evaluation_error, evaluation_accuracy = fast_adapt(batch,
                                                                   learner,
                                                                   criterion,
                                                                   fas)
                iteration_error += evaluation_error
                iteration_acc += evaluation_accuracy

            iteration_error /= tps
            iteration_acc /= tps
            print('Iteration {}, Loss : {:.3f} Acc : {:.3f}'.format(iteration, iteration_error.item(), iteration_acc.item()))

            # Take the meta-learning step
            opt.zero_grad()
            iteration_error.backward()
            opt.step()

        torch.save({'epoch': iteration+1,
                    'state_dict': meta_model.state_dict(),
                    'optimizer': opt.state_dict()},
                   args.outdir + '/model_checkpoint.pth.tar')
        shutil.copyfile(args.outdir + '/model_checkpoint.pth.tar', args.outdir + '/model_best.pth.tar')

        print(80 * '=')

        ## MODEL EVALUATION LOGGING ##
        data_loaders = get_dataloader(args.database_dir, args.train_dir, args.valid_dir, args.test_dir,
                                      args.batch_size, args.num_workers)
        checkpoint = torch.load(args.outdir + '/model_best.pth.tar')
        meta_model.load_state_dict(checkpoint['state_dict'])
        EER = test(meta_model, data_loaders['test'], iteration, is_graph=True)

        header = ['weight_decay', 'learning_rate', 'scale', 'margin', 'type', 'batch_size', 'embedding_size', 'EER', 'out_dir']
        info = [args.weight_decay, args.learning_rate, args.scale_rate, args.margin, args.type, args.batch_size, args.embedding_size, EER, args.outdir]

        if not os.path.exists(args.logdir):
            with open(args.logdir, 'w') as file:
                logger = csv.writer(file)
                logger.writerow(header)
                logger.writerow(info)
        else:
            with open(args.logdir, 'a') as file:
                logger = csv.writer(file)
                logger.writerow(info)


def test(model, dataloader, epoch, is_graph=False):
    global best_test
    labels, distances = [], []
    with torch.set_grad_enabled(False):
        comparer = FullPairComparer().cuda()
        model.eval()
        for batch_idx, (data1, data2, target) in enumerate(dataloader):
            dist = []
            target = target.cuda(non_blocking=True)

            output1 = model(data1, False)
            output2 = model(data2, False)
            dist = comparer(output1, output2) #TODO: sign - torch.sign()
            #dist = comparer(torch.sign(F.relu(output1)), torch.sign(F.relu(output2)))  # TODO: sign - torch.sign()
            distances.append(dist.data.cpu().numpy())
            labels.append(target.data.cpu().numpy())
            if batch_idx % 50 == 0:
                print('Batch-Index -{}'.format(str(batch_idx)))

    labels = np.array([sublabel for label in labels for sublabel in label])
    distances = np.array([subdist for dist in distances for subdist in dist])
    tpr, fpr, fnr, fpr_optimum, fnr_optimum, accuracy, threshold = evaluate(distances, labels)
    print(labels[0:4])
    print(distances[0:4])

    EER = np.mean(fpr_optimum + fnr_optimum) / 2
    print('TEST - Accuracy           = {:.12f}'.format(accuracy))
    print('TEST - EER                = {:.12f}'.format(EER))
    print('Best threshold            = {:.12f}'.format(threshold))
    is_best = EER <= best_test
    best_test = min(EER, best_test)

    if is_best and is_graph:
        plot_roc(fpr, tpr, figure_name=args.outdir + '/Test_ROC-{}.png'.format(epoch))
        plot_DET_with_EER(fpr, fnr, fpr_optimum, fnr_optimum,
                          figure_name=args.outdir + '/Test_DET-{}.png'.format(epoch))
        plot_density(distances, labels, figure_name=args.outdir + '/Test_DENSITY-{}.png'.format(epoch))
        df_results = pd.DataFrame({'distances': distances.transpose(), 'labels': labels.transpose()})
        df_results.to_csv(args.outdir + "/test_outputs.csv", index=False)

        if args.evaluate is False:
            shutil.copyfile(args.outdir + '/model_best.pth.tar', args.outdir + '/test_model_best.pth.tar')

    return EER


if __name__ == '__main__':
    main()
